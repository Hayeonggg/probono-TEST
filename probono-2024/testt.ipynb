{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#라이브러리 설치\n",
    "pip install -U accelerate==0.29.3  peft==0.10.0  bitsandbytes==0.43.1  transformers==4.40.1  trl==0.8.6  datasets==2.19.0\n",
    "\n",
    "#accelerate : 파이토치 모델의 학습 속도 향상, 최적화 위한 라이브러리\n",
    "#peft : parameter efficient fine tuning\n",
    "#bitsandbytes : 모델 매개변수 양자화. 메모리 사용량 절감\n",
    "#trl: Transformer Reinforcement Learning / 강화학습 기반 언어모델 미세조정 기술\n",
    "#datasets : 자연어 처리 데이터셋 다운로드, 전처리 지원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install datasets\n",
    "\n",
    "\n",
    "\n",
    "pip install peft\n",
    "pip install -i https://pypi.org/simple/ bitsandbytes\n",
    "pip install transformers\n",
    "pip install accelerate\n",
    "pip install trl\n",
    "python -m pip install --upgrade pip\n",
    "pip install ipywidgets --upgrade\n",
    "\n",
    "#pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaMA 기본"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "#라이브러리 로드\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import(\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "\n",
    "import huggingface_hub\n",
    "huggingface_hub.login('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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 설정\n",
    "base_model = \"beomi/Llama-3-Open-Ko-8B\"# beomi님의 Llama3 한국어 파인튜닝 모델\n",
    "generate_dataset = '/content/test_dataset.csv'\n",
    "\n",
    "#새로운 모델 이름\n",
    "new_model = 'Llama-3-Open-Ko-8B-generate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 18\u001b[0m\n\u001b[1;32m     10\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m     11\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 모델 로드\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBASE_MODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;66;43;03m#GPU 사용\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(BASE_MODEL)\u001b[38;5;66;03m#, add_special_tokens=True\u001b[39;00m\n\u001b[1;32m     24\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3957\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3954\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(model, dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[1;32m   3956\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3957\u001b[0m         \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3959\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3960\u001b[0m     model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py:102\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m--> 102\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    103\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from_pretrained`. Check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m         )\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, TrainingArguments\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "#BASE_MODEL = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "BASE_MODEL = \"beomi/Llama-3-Open-Ko-8B\"\n",
    "\n",
    "# 4bit 양자화 설정\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL,\n",
    "                                             quantization_config=bnb_config,\n",
    "                                             device_map=\"auto\"#GPU 사용\n",
    "                                            )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)#, add_special_tokens=True\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 프롬프트 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keywords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 프롬프트 모음\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#1차 프롬프트\u001b[39;00m\n\u001b[1;32m      4\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m다음 키워드를 사용하여 문장을 작성해 :\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m                      키워드) 고양이, 침대, 자다 -> 생성문장) 고양이가 침대에서 자고 있는 사진입니다.\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;124m                      키워드) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mkeywords\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> 생성문장)\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#2차 프롬프트\u001b[39;00m\n\u001b[1;32m      9\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m다음 키워드에 집중하여 문장을 작성해 :\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m                          키워드) 고양이, 침대, 자다 -> 생성문장) 고양이가 침대에서 자고 있는 사진입니다.\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m                          키워드) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeywords\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> 생성문장)\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keywords' is not defined"
     ]
    }
   ],
   "source": [
    "# 프롬프트 모음\n",
    "\n",
    "#1차 프롬프트\n",
    "prompt = f\"\"\"다음 키워드를 사용하여 문장을 작성해 :\n",
    "                      키워드) 고양이, 침대, 자다 -> 생성문장) 고양이가 침대에서 자고 있는 사진입니다.\n",
    "                      키워드) {keywords} -> 생성문장)\"\"\"\n",
    "\n",
    "#2차 프롬프트\n",
    "prompt = f\"\"\"다음 키워드에 집중하여 문장을 작성해 :\n",
    "                          키워드) 고양이, 침대, 자다 -> 생성문장) 고양이가 침대에서 자고 있는 사진입니다.\n",
    "                          키워드) {keywords} -> 생성문장)\"\"\"\n",
    "\n",
    "#3차 프롬프트\n",
    "prompt = f\"\"\"다음 제시된 키워드를 모두 사용하여 사진을 설명하듯 문장을 1개 작성해 :\n",
    "                          키워드) 고양이, 침대, 자다 -> 생성문장) 고양이가 침대에서 자고 있는 사진입니다.\n",
    "                          키워드) {keywords} -> 생성문장)\"\"\"\n",
    "\n",
    "#4차 프롬프트\n",
    "prompt = f\"\"\"다음 제시된 키워드를 반드시 전부 사용하여, 사진을 묘사하듯, 문장을 1개 작성하고, 반드시 사진입니다.로 문장이 끝나게 해 :\n",
    "                              키워드) 고양이, 침대, 자다 -> 생성문장) 고양이가 침대에서 자고 있는 사진입니다.\n",
    "                              키워드) {keywords} -> 생성문장)\"\"\"\n",
    "\n",
    "#5차 프롬프트\n",
    "prompt = f\"\"\"다음 제시된 키워드를 반드시 모두 사용하여, 사진을 묘사하듯, 문장을 1개 작성하고, 문장을 자연스럽고, 수식어를 약간 추가하고, 사진입니다.로 문장이 끝나게 해 :\n",
    "                              키워드) 고양이, 침대, 자다 -> 생성문장) 고양이가 침대에서 자고 있는 사진입니다.\n",
    "                              키워드) {keywords} -> 생성문장)\"\"\"\n",
    "                              \n",
    "#6차 프롬프트\n",
    "\n",
    "prompt = f\"\"\"다음 제시된 키워드를 '모두' 사용하여, 사진을 묘사하듯 자연스럽게 문장을 1개 작성하고, 수식어를 1개 추가하고, 사진입니다.로 문장이 끝나게 해 :\n",
    "            키워드) 고양이, 침대, 자다 -> 생성문장) 고양이가 침대에서 자고 있는 사진입니다. \n",
    "            키워드) {keywords} -> 생성문장)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SentenceTransformer' object has no attribute 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 프롬프트 튜닝\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_simple_text\u001b[39m(keywords):\n\u001b[1;32m      5\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m다음 제시된 키워드를 반드시 모두 사용하여, 사진을 묘사하듯, 문장을 1개 작성하고, 문장을 자연스럽고, 수식어를 약간 추가하고, 사진입니다.로 문장이 끝나게 해 :\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m                              키워드) 고양이, 침대, 자다 -> 생성문장) 고양이가 침대에서 자고 있는 사진입니다.\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m                              키워드) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeywords\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> 생성문장)\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py:906\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    895\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m    896\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m infer_framework_load_model(\n\u001b[1;32m    897\u001b[0m         model,\n\u001b[1;32m    898\u001b[0m         model_classes\u001b[38;5;241m=\u001b[39mmodel_classes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m    904\u001b[0m     )\n\u001b[0;32m--> 906\u001b[0m model_config \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\n\u001b[1;32m    907\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[1;32m    908\u001b[0m load_tokenizer \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28mtype\u001b[39m(model_config) \u001b[38;5;129;01min\u001b[39;00m TOKENIZER_MAPPING\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m model_config\u001b[38;5;241m.\u001b[39mtokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    911\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tokenizer, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    912\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SentenceTransformer' object has no attribute 'config'"
     ]
    }
   ],
   "source": [
    "# 프롬프트 튜닝\n",
    "pipe = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "def generate_simple_text(keywords):\n",
    "    prompt = f\"\"\"다음 제시된 키워드를 반드시 모두 사용하여, 사진을 묘사하듯, 문장을 1개 작성하고, 문장을 자연스럽고, 수식어를 약간 추가하고, 사진입니다.로 문장이 끝나게 해 :\n",
    "                              키워드) 고양이, 침대, 자다 -> 생성문장) 고양이가 침대에서 자고 있는 사진입니다.\n",
    "                              키워드) {keywords} -> 생성문장)\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    generated_text = outputs[0]['generated_text'].strip()\n",
    "      # '생성문장)' 이후의 텍스트만 추출\n",
    "    if \"생성문장)\" in generated_text:\n",
    "        generated_sentences = generated_text.split(\"생성문장)\")[1:]  # 첫 번째 생성문장 이후의 텍스트들을 리스트로 나눔\n",
    "        if len(generated_sentences) > 1:\n",
    "            generated_sentence = generated_sentences[1].strip().split(\".\")[0].strip() + \".\"\n",
    "        else:\n",
    "            generated_sentence = generated_sentences[0].strip().split(\".\")[0].strip() + \".\"\n",
    "    else:\n",
    "        generated_sentence = generated_text  # 예외 처리로 전체 텍스트 반환\n",
    "\n",
    "    return generated_sentence\n",
    "\n",
    "\n",
    "\n",
    "# 1줄 설명 생성\n",
    "test_keywords = [ \"잔디, 꽃, 핀다\",\n",
    "                              \"2마리, 레서판다, 서있다, 앞발을 들다\",\n",
    "                              \"2명, 사람, 아이스크림, 미소, 함께 있다\",\n",
    "                              \"5명, 해변, 웃고있다, 포즈\",\n",
    "                              \"6명, 사무실, 컴퓨터, 웃고있다\",\n",
    "                              \"많은 사람, 발표, 회의실, 앉아있다\",\n",
    "                              \"사람, 개, 인사, 야외\",\n",
    "                              \"많은 소, 목장, 풀밭, 빨간 헛간\",\n",
    "                              \"나무길, 벤치, 노란잎\",\n",
    "                              \"버스, 정류장, 사람들, 줄서다\",\n",
    "                              \"파스타, 접시, 마늘, 새우\",\n",
    "                              \"한식, 고기구이, 상차림, 반찬\",\n",
    "                              \"한옥, 마당, 나무\",\n",
    "                              \"고층건물, 아파트, 도시, 하늘\",\n",
    "                              \"남성, 흰 셔츠, 청바지, 가방\",\n",
    "                              \"여성, 체크셔츠, 청바지, 셀카\",\n",
    "                              \"그림, 전시, 액자, 미술작품\",\n",
    "                              \"축구, 선수, 경기, 공, 차다\",\n",
    "                              \"사람들, 회의실, 화상회의, 마스크\",\n",
    "                              \"사람, 회의실, 화상회의, 모니터\",\n",
    "                              \"사람, 노트북, 스마트폰\",\n",
    "                              \"마우스, 키보드, 펜, 노트\"]\n",
    "\n",
    "for test in test_keywords:\n",
    "  description = generate_simple_text(test)\n",
    " #print(f\"키워드 : {test}  ->  생성문장 : {description}\")\n",
    "  print(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 프롬프트 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install bert-score\n",
    "pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = ['잔디 위에 꽃이 피어 있습니다.',\n",
    "            '2마리의 레서판다가 앞발을 들고 서 있습니다.',\n",
    "            '2사람이 함께 아이스크림을 먹으며 미소 짓고 있습니다.',\n",
    "            '5명의 사람들이 해변에서 웃으며 포즈를 취하고 있습니다.',\n",
    "            '6명의 사람들이 사무실에서 컴퓨터를 보며 웃고 있습니다.',\n",
    "            '많은 사람들이 회의실에서 발표를 들으며 앉아있습니다.',\n",
    "            '사람이 야외에서 개와 인사하는 듯한 모습을 보이고 있습니다.',\n",
    "            '소들이 목장의 풀밭에 있으며, 뒤에는 빨간 헛간이 있습니다.',\n",
    "            '노란잎이 가득한 나무길에 벤치가 놓여 있습니다.',\n",
    "            '사람들이 버스 정류장에서 버스를 타기 위해 줄을 서고 있습니다.',\n",
    "            '새우와 마늘이 들어간 파스타가 접시에 담겨 있습니다.',\n",
    "            '다양한 한식 반찬과 고기구이가 준비된 상차림이 있습니다.',\n",
    "            '나무로 지어진 한옥과 마당이 있습니다.',\n",
    "            '고층 아파트 건물이 맑은 하늘 아래에 있습니다.',\n",
    "            '한 남성이 흰 셔츠와 청바지를 입고 가방을 들고 있습니다.',\n",
    "            '한 여성이 체크셔츠와 청바지를 입고 셀카를 찍고 있습니다.',\n",
    "            '미술작품들이 액자에 담겨 전시되고 있습니다.',\n",
    "            '축구 선수들이 경기 중에 공을 차고 있습니다.',\n",
    "            '사람들이 회의실에서 마스크를 쓰고 화상회의를 진행하고 있습니다.',\n",
    "            '한 사람이 회의실에서 모니터를 통해 화상회의를 진행하고 있습니다.',\n",
    "            '한 사람이 노트북으로 작업을 하고 있으며 옆에 스마트폰이 놓여 있습니다.',\n",
    "            '마우스와 키보드, 펜, 그리고 노트가 책상 위에 놓여 있습니다.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = [\n",
    "    '잔디에 핀 꽃 사진입니다.',\n",
    "    '2마리의 레서판다들이 서서 앞발을 들고 있는 사진입니다.',\n",
    "    '2명이 사람과 함께 아이스크림을 먹고 있는 사진입니다.',\n",
    "    '해변에서 웃고있는 5명의 사진입니다.',\n",
    "    '6명이 사무실에서 컴퓨터를 보며 웃고 있는 사진입니다.',\n",
    "    '많은 사람들이 발표를 하고 있는 회의실의 사진입니다.',\n",
    "    '사람과 개가 야외에서 인사하고 있는 사진입니다.',\n",
    "    '많은 소들이 목장의 풀밭에 서 있는 사진입니다.',\n",
    "    '나무길 벤치에 앉아 있는 노란 잎이 있는 사진입니다.',\n",
    "    '버스정류장에 사람들이 줄을 서 있는 사진입니다.',\n",
    "    '파스타를 접시에 담아 마늘과 새우를 넣어 먹는 사진입니다.',\n",
    "    '한식 고기구이가 상에 올려져 있고 반찬이 가득 담겨 있는 사진입니다.',\n",
    "    '한옥의 마당에 나무가 있는 사진입니다.',\n",
    "    '고층건물이 도시 하늘 위로 아파트를 뽑아 올리는 사진입니다.',\n",
    "    '남성이 흰 셔츠를 입고 청바지를 입고 가방을 들고 있는 사진입니다.',\n",
    "    '여성이 청바지와 체크셔츠를 입고 셀카를 찍고 있는 사진입니다.',\n",
    "    '그림을 전시할 때 사용하는 액자에 있는 미술작품 사진입니다.',\n",
    "    '축구 선수가 공을 차는 경기 사진입니다.',\n",
    "    '사람들이 회의실에서 화상회의를 하고 있는 사진입니다.',\n",
    "    '사람이 모니터를 보며 화상회의를 하고 있는 사진입니다.',\n",
    "    '사람이 노트북과 스마트폰을 들고 있는 사진입니다.',\n",
    "    '마우스, 키보드, 펜, 노트가 놓여 있는 사진입니다.'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b010340c6ae4762aea88209df8e656c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f998bb1953249cc950af13e7424f751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.09 seconds, 240.24 sentences/sec\n",
      "Semantic Similarity: 0.8057155013084412\n",
      "\n",
      "BERTScore (F1): 0.8457323312759399\n",
      "BERTScore (Precision): 0.8479255437850952\n",
      "BERTScore (Recall): 0.8438053131103516\n"
     ]
    }
   ],
   "source": [
    "#2) 정량적 지표 2\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from bert_score import score\n",
    "\n",
    "# Sentence-BERT 모델 불러오기\n",
    "#model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "model = SentenceTransformer('jhgan/ko-sroberta-multitask')#https://github.com/jhgan00/ko-sentence-transformers\n",
    "\n",
    "# STS 점수를 저장할 리스트\n",
    "sts_scores = []\n",
    "\n",
    "# 각 문장 쌍에 대해 STS 점수 계산\n",
    "for ref, cand in zip(references, candidates):\n",
    "    reference_embedding = model.encode(ref, convert_to_tensor=True)\n",
    "    candidate_embedding = model.encode(cand, convert_to_tensor=True)\n",
    "    similarity = util.pytorch_cos_sim(reference_embedding, candidate_embedding)\n",
    "    sts_scores.append(similarity.item())\n",
    "\n",
    "# 평균 STS 점수 계산\n",
    "avg_sts_score = sum(sts_scores) / len(sts_scores)\n",
    "\n",
    "\n",
    "\n",
    "                                        \n",
    "# BERTScore 계산\n",
    "P, R, F1 = score(candidates, references, lang=\"ko\", verbose=True)\n",
    "\n",
    "# 평균 BERTScore F1 점수 계산\n",
    "avg_f1_bert_score = F1.mean().item()\n",
    "avg_P_bert_score = P.mean().item()#정밀도\n",
    "avg_R_bert_score = R.mean().item()#재현율\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Semantic Similarity: {similarity.item()}\\n\")\n",
    "print(f\"BERTScore (F1): {avg_f1_bert_score}\")\n",
    "print(f\"BERTScore (Precision): {avg_P_bert_score}\")\n",
    "print(f\"BERTScore (Recall): {avg_R_bert_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.03288761866329751\n",
      "\n",
      "ROUGE Score: 0.29037572965058495\n",
      "\n",
      "Cosine Similarity: 0.20517251398392003\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "#정량적 지표 1\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ROUGE 초기화\n",
    "rouge = Rouge()\n",
    "\n",
    "# BLEU, ROUGE, 코사인 유사도 점수 저장용 리스트\n",
    "bleu_scores = []\n",
    "rouge_scores = []\n",
    "cosine_similarities = []\n",
    "\n",
    "for ref, cand in zip(references, candidates):\n",
    "    # 1) BLEU Score\n",
    "    bleu_score = sentence_bleu([ref.split()], cand.split())\n",
    "    bleu_scores.append(bleu_score)\n",
    "    \n",
    "    # 2) ROUGE Score\n",
    "    rouge_score = rouge.get_scores(cand, ref)[0]['rouge-l']['f']  # 'rouge-l'의 F1 점수를 사용\n",
    "    rouge_scores.append(rouge_score)\n",
    "    \n",
    "    # 3) Cosine Similarity\n",
    "    sentences = [ref, cand]\n",
    "    vectorizer = TfidfVectorizer().fit_transform(sentences)\n",
    "    vectors = vectorizer.toarray()\n",
    "    cosine_sim = cosine_similarity([vectors[0]], [vectors[1]])[0][0]\n",
    "    cosine_similarities.append(cosine_sim)\n",
    "\n",
    "# 평균 계산\n",
    "avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "avg_rouge = sum(rouge_scores) / len(rouge_scores)\n",
    "avg_cosine_similarity = sum(cosine_similarities) / len(cosine_similarities)\n",
    "\n",
    "print(f\"BLEU Score: {avg_bleu}\\n\")\n",
    "print(f\"ROUGE Score: {avg_rouge}\\n\")\n",
    "print(f\"Cosine Similarity: {avg_cosine_similarity}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
