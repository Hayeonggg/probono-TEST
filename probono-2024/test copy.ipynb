{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#라이브러리 설치\n",
    "pip install -U accelerate==0.29.3  peft==0.10.0  bitsandbytes==0.43.1  transformers==4.40.1  trl==0.8.6  datasets==2.19.0\n",
    "\n",
    "#accelerate : 파이토치 모델의 학습 속도 향상, 최적화 위한 라이브러리\n",
    "#peft : parameter efficient fine tuning\n",
    "#bitsandbytes : 모델 매개변수 양자화. 메모리 사용량 절감\n",
    "#trl: Transformer Reinforcement Learning / 강화학습 기반 언어모델 미세조정 기술\n",
    "#datasets : 자연어 처리 데이터셋 다운로드, 전처리 지원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install datasets\n",
    "d:\\DIP\\PYTHON\\P3.9.7\\python.exe -m pip install tokenizers==0.13.3\n",
    "\n",
    "d:\\DIP\\PYTHON\\P3.9.7\\python.exe -m pip install datasets\n",
    "d:\\DIP\\PYTHON\\P3.9.7\\python.exe -m pip install peft\n",
    "d:\\DIP\\PYTHON\\P3.9.7\\python.exe -m pip install -i https://pypi.org/simple/ bitsandbytes\n",
    "d:\\DIP\\PYTHON\\P3.9.7\\python.exe -m pip install transformers -U\n",
    "d:\\DIP\\PYTHON\\P3.9.7\\python.exe -m pip install accelerate\n",
    "d:\\DIP\\PYTHON\\P3.9.7\\python.exe -m pip install trl\n",
    "\n",
    "\n",
    "pip install peft\n",
    "pip install -i https://pypi.org/simple/ bitsandbytes\n",
    "pip install transformers\n",
    "pip install accelerate\n",
    "pip install trl\n",
    "python -m pip install --upgrade pip\n",
    "pip install ipywidgets --upgrade\n",
    "\n",
    "#pip install -U bitsandbytes\n",
    "d:\\DIP\\PYTHON\\P3.9.7\\python.exe -m pip install ipywidgets --upgrade\n",
    "d:\\DIP\\PYTHON\\P3.9.7\\python.exe -m pip install jupyter --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaMA 기본"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "#라이브러리 로드\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import(\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "\n",
    "import huggingface_hub\n",
    "huggingface_hub.login('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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 설정\n",
    "base_model = \"beomi/Llama-3-Open-Ko-8B\"# beomi님의 Llama3 한국어 파인튜닝 모델\n",
    "generate_dataset = '/content/test_dataset.csv'\n",
    "\n",
    "#새로운 모델 이름\n",
    "new_model = 'Llama-3-Open-Ko-8B-generate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LlamaForCausalLM.__init__() got an unexpected keyword argument 'load_in_8bit_fp32_cpu_offload'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m\n\u001b[1;32m     10\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m     11\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 모델 로드\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBASE_MODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mload_in_8bit_fp32_cpu_offload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;66;43;03m#GPU 사용\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(BASE_MODEL)\u001b[38;5;66;03m#, add_special_tokens=True\u001b[39;00m\n\u001b[1;32m     25\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3832\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3826\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[1;32m   3827\u001b[0m     config, use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map\n\u001b[1;32m   3828\u001b[0m )\n\u001b[1;32m   3830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m   3831\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[0;32m-> 3832\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3834\u001b[0m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[1;32m   3835\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "\u001b[0;31mTypeError\u001b[0m: LlamaForCausalLM.__init__() got an unexpected keyword argument 'load_in_8bit_fp32_cpu_offload'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, TrainingArguments\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "#BASE_MODEL = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "BASE_MODEL = \"beomi/Llama-3-Open-Ko-8B\"\n",
    "\n",
    "# 4bit 양자화 설정\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL,\n",
    "                                             quantization_config=bnb_config,\n",
    "                                             device_map=\"auto\"#GPU 사용\n",
    "                                            )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, add_special_tokens=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 프롬프트 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 프롬프트 튜닝\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39m\u001b[43mtokenizer\u001b[49m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_simple_text\u001b[39m(keywords):\n\u001b[1;32m      5\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m다음 제시된 키워드를 반드시 모두 사용하여, 사진을 묘사하듯, 문장을 1개 작성하고, 문장을 자연스럽고, 수식어를 약간 추가하고, 사진입니다.로 문장이 끝나게 해 :\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m                              키워드) 고양이, 침대, 자다 -> 생성문장) 고양이가 침대에서 자고 있는 사진입니다.\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m                              키워드) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeywords\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> 생성문장)\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# 프롬프트 튜닝\n",
    "pipe = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "def generate_simple_text(keywords):\n",
    "    prompt = f\"\"\"다음 제시된 키워드를 반드시 모두 사용하여, 사진을 묘사하듯, 문장을 1개 작성하고, 문장을 자연스럽고, 수식어를 약간 추가하고, 사진입니다.로 문장이 끝나게 해 :\n",
    "                              키워드) 고양이, 침대, 자다 -> 생성문장) 고양이가 침대에서 자고 있는 사진입니다.\n",
    "                              키워드) {keywords} -> 생성문장)\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.8,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    generated_text = outputs[0]['generated_text'].strip()\n",
    "      # '생성문장)' 이후의 텍스트만 추출\n",
    "    if \"생성문장)\" in generated_text:\n",
    "        generated_sentences = generated_text.split(\"생성문장)\")[1:]  # 첫 번째 생성문장 이후의 텍스트들을 리스트로 나눔\n",
    "        if len(generated_sentences) > 1:\n",
    "            generated_sentence = generated_sentences[1].strip().split(\".\")[0].strip() + \".\"\n",
    "        else:\n",
    "            generated_sentence = generated_sentences[0].strip().split(\".\")[0].strip() + \".\"\n",
    "    else:\n",
    "        generated_sentence = generated_text  # 예외 처리로 전체 텍스트 반환\n",
    "\n",
    "    return generated_sentence\n",
    "\n",
    "\n",
    "\n",
    "# 1줄 설명 생성\n",
    "test_keywords = [ \"잔디, 꽃, 핀다\",\n",
    "                              \"2마리, 레서판다, 서있다, 앞발을 들다\",\n",
    "                              \"2명, 사람, 아이스크림, 미소, 함께 있다\",\n",
    "                              \"5명, 해변, 웃고있다, 포즈\",\n",
    "                              \"6명, 사무실, 컴퓨터, 웃고있다\",\n",
    "                              \"많은 사람, 발표, 회의실, 앉아있다\",\n",
    "                              \"사람, 개, 인사, 야외\",\n",
    "                              \"많은 소, 목장, 풀밭, 빨간 헛간\",\n",
    "                              \"나무길, 벤치, 노란잎\",\n",
    "                              \"버스, 정류장, 사람들, 줄서다\",\n",
    "                              \"파스타, 접시, 마늘, 새우\",\n",
    "                              \"한식, 고기구이, 상차림, 반찬\",\n",
    "                              \"한옥, 마당, 나무\",\n",
    "                              \"고층건물, 아파트, 도시, 하늘\",\n",
    "                              \"남성, 흰 셔츠, 청바지, 가방\",\n",
    "                              \"여성, 체크셔츠, 청바지, 셀카\",\n",
    "                              \"그림, 전시, 액자, 미술작품\",\n",
    "                              \"축구, 선수, 경기, 공, 차다\",\n",
    "                              \"사람들, 회의실, 화상회의, 마스크\",\n",
    "                              \"사람, 회의실, 화상회의, 모니터\",\n",
    "                              \"사람, 노트북, 스마트폰\",\n",
    "                              \"마우스, 키보드, 펜, 노트\"]\n",
    "\n",
    "for test in test_keywords:\n",
    "  description = generate_simple_text(test)\n",
    " #print(f\"키워드 : {test}  ->  생성문장 : {description}\")\n",
    "  print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 모음\n",
    "\n",
    "#1차 프롬프트\n",
    "prompt = f\"\"\"다음 키워드를 사용하여 문장을 작성해 :\n",
    "                      키워드) 고양이, 침대, 자다 -> 생성문장) 고양이가 침대에서 자고 있는 사진입니다.\n",
    "                      키워드) {keywords} -> 생성문장)\"\"\"\n",
    "\n",
    "#2차 프롬프트\n",
    "prompt = f\"\"\"다음 키워드에 집중하여 문장을 작성해 :\n",
    "                          키워드) 고양이, 침대, 자다 -> 생성문장) 고양이가 침대에서 자고 있는 사진입니다.\n",
    "                          키워드) {keywords} -> 생성문장)\"\"\"\n",
    "\n",
    "#3차 프롬프트\n",
    "prompt = f\"\"\"다음 제시된 키워드를 모두 사용하여 사진을 설명하듯 문장을 1개 작성해 :\n",
    "                          키워드) 고양이, 침대, 자다 -> 생성문장) 고양이가 침대에서 자고 있는 사진입니다.\n",
    "                          키워드) {keywords} -> 생성문장)\"\"\"\n",
    "\n",
    "#4차 프롬프트\n",
    "prompt = f\"\"\"다음 제시된 키워드를 반드시 전부 사용하여, 사진을 묘사하듯, 문장을 1개 작성하고, 반드시 사진입니다.로 문장이 끝나게 해 :\n",
    "                              키워드) 고양이, 침대, 자다 -> 생성문장) 고양이가 침대에서 자고 있는 사진입니다.\n",
    "                              키워드) {keywords} -> 생성문장)\"\"\"\n",
    "\n",
    "#5차 프롬프트\n",
    "prompt = f\"\"\"다음 제시된 키워드를 반드시 모두 사용하여, 사진을 묘사하듯, 문장을 1개 작성하고, 문장을 자연스럽고, 수식어를 약간 추가하고, 사진입니다.로 문장이 끝나게 해 :\n",
    "                              키워드) 고양이, 침대, 자다 -> 생성문장) 고양이가 침대에서 자고 있는 사진입니다.\n",
    "                              키워드) {keywords} -> 생성문장)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
